{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7657cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np #Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af0f756b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1567, 592)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('signal-data.csv')\n",
    "print(df.shape) #592 columns with only 1567 rows, extremely high dimensionality can be seen here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9184219f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>...</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>100.0</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>1.3424</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>...</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>1.4333</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>1.4622</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>...</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>100.0</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0        1          2          3       4      5         6       7  \\\n",
       "0     3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   97.6133  0.1242   \n",
       "1     3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0  102.3433  0.1247   \n",
       "2     2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   95.4878  0.1241   \n",
       "3     2988.72  2479.90  2199.0333   909.7926  1.3204  100.0  104.2367  0.1217   \n",
       "4     3032.24  2502.87  2233.3667  1326.5200  1.5334  100.0  100.3967  0.1235   \n",
       "...       ...      ...        ...        ...     ...    ...       ...     ...   \n",
       "1562  2899.41  2464.36  2179.7333  3085.3781  1.4843  100.0   82.2467  0.1248   \n",
       "1563  3052.31  2522.55  2198.5667  1124.6595  0.8763  100.0   98.4689  0.1205   \n",
       "1564  2978.81  2379.78  2206.3000  1110.4967  0.8236  100.0   99.4122  0.1208   \n",
       "1565  2894.92  2532.01  2177.0333  1183.7287  1.5726  100.0   98.7978  0.1213   \n",
       "1566  2944.92  2450.76  2195.4444  2914.1792  1.5978  100.0   85.1011  0.1235   \n",
       "\n",
       "           8       9  ...       581     582     583     584      585     586  \\\n",
       "0     1.5005  0.0162  ...       NaN  0.5005  0.0118  0.0035   2.3630     NaN   \n",
       "1     1.4966 -0.0005  ...  208.2045  0.5019  0.0223  0.0055   4.4447  0.0096   \n",
       "2     1.4436  0.0041  ...   82.8602  0.4958  0.0157  0.0039   3.1745  0.0584   \n",
       "3     1.4882 -0.0124  ...   73.8432  0.4990  0.0103  0.0025   2.0544  0.0202   \n",
       "4     1.5031 -0.0031  ...       NaN  0.4800  0.4766  0.1045  99.3032  0.0202   \n",
       "...      ...     ...  ...       ...     ...     ...     ...      ...     ...   \n",
       "1562  1.3424 -0.0045  ...  203.1720  0.4988  0.0143  0.0039   2.8669  0.0068   \n",
       "1563  1.4333 -0.0061  ...       NaN  0.4975  0.0131  0.0036   2.6238  0.0068   \n",
       "1564     NaN     NaN  ...   43.5231  0.4987  0.0153  0.0041   3.0590  0.0197   \n",
       "1565  1.4622 -0.0072  ...   93.4941  0.5004  0.0178  0.0038   3.5662  0.0262   \n",
       "1566     NaN     NaN  ...  137.7844  0.4987  0.0181  0.0040   3.6275  0.0117   \n",
       "\n",
       "         587     588       589  Pass/Fail  \n",
       "0        NaN     NaN       NaN         -1  \n",
       "1     0.0201  0.0060  208.2045         -1  \n",
       "2     0.0484  0.0148   82.8602          1  \n",
       "3     0.0149  0.0044   73.8432         -1  \n",
       "4     0.0149  0.0044   73.8432         -1  \n",
       "...      ...     ...       ...        ...  \n",
       "1562  0.0138  0.0047  203.1720         -1  \n",
       "1563  0.0138  0.0047  203.1720         -1  \n",
       "1564  0.0086  0.0025   43.5231         -1  \n",
       "1565  0.0245  0.0075   93.4941         -1  \n",
       "1566  0.0162  0.0045  137.7844         -1  \n",
       "\n",
       "[1567 rows x 591 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('Time',axis=1) #Dropping the Time column, as the timestamp is not required in training the model\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43eec4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 591 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.dropna()) #There are NaN values in each row of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e19614c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    1463\n",
       " 1     104\n",
       "Name: Pass/Fail, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Pass/Fail'].value_counts() #Huge disparity between the target value counts\n",
    "#This might result into training a biased model, therefore it requires specific upsampling or downsampling methods for target balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd0e6e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since it is not viable to explore each and every variable just for the sake of the treatment of NaN values,\n",
    "#let's check how many variables are not having at least 1400 not null values out of total 1567 values.\n",
    "l1=[]\n",
    "for i in list(df.columns):\n",
    "    if len(df[df[i].notnull()]) < 1400:\n",
    "        l1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e49c25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['72',\n",
       " '73',\n",
       " '85',\n",
       " '109',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '157',\n",
       " '158',\n",
       " '220',\n",
       " '244',\n",
       " '245',\n",
       " '246',\n",
       " '247',\n",
       " '292',\n",
       " '293',\n",
       " '345',\n",
       " '346',\n",
       " '358',\n",
       " '382',\n",
       " '383',\n",
       " '384',\n",
       " '385',\n",
       " '492',\n",
       " '516',\n",
       " '517',\n",
       " '518',\n",
       " '519',\n",
       " '546',\n",
       " '547',\n",
       " '548',\n",
       " '549',\n",
       " '550',\n",
       " '551',\n",
       " '552',\n",
       " '553',\n",
       " '554',\n",
       " '555',\n",
       " '556',\n",
       " '557',\n",
       " '562',\n",
       " '563',\n",
       " '564',\n",
       " '565',\n",
       " '566',\n",
       " '567',\n",
       " '568',\n",
       " '569',\n",
       " '578',\n",
       " '579',\n",
       " '580',\n",
       " '581']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 #These columns don;t have at least 1400 non null values, therefore it's better to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf73f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(l1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5110a7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>577</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>100.0</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>1.3424</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>...</td>\n",
       "      <td>11.7256</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>1.4333</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>...</td>\n",
       "      <td>17.8379</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17.7267</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>1.4622</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>...</td>\n",
       "      <td>19.2104</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>100.0</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>22.9183</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 539 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0        1          2          3       4      5         6       7  \\\n",
       "0     3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   97.6133  0.1242   \n",
       "1     3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0  102.3433  0.1247   \n",
       "2     2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   95.4878  0.1241   \n",
       "3     2988.72  2479.90  2199.0333   909.7926  1.3204  100.0  104.2367  0.1217   \n",
       "4     3032.24  2502.87  2233.3667  1326.5200  1.5334  100.0  100.3967  0.1235   \n",
       "...       ...      ...        ...        ...     ...    ...       ...     ...   \n",
       "1562  2899.41  2464.36  2179.7333  3085.3781  1.4843  100.0   82.2467  0.1248   \n",
       "1563  3052.31  2522.55  2198.5667  1124.6595  0.8763  100.0   98.4689  0.1205   \n",
       "1564  2978.81  2379.78  2206.3000  1110.4967  0.8236  100.0   99.4122  0.1208   \n",
       "1565  2894.92  2532.01  2177.0333  1183.7287  1.5726  100.0   98.7978  0.1213   \n",
       "1566  2944.92  2450.76  2195.4444  2914.1792  1.5978  100.0   85.1011  0.1235   \n",
       "\n",
       "           8       9  ...      577     582     583     584      585     586  \\\n",
       "0     1.5005  0.0162  ...  14.9509  0.5005  0.0118  0.0035   2.3630     NaN   \n",
       "1     1.4966 -0.0005  ...  10.9003  0.5019  0.0223  0.0055   4.4447  0.0096   \n",
       "2     1.4436  0.0041  ...   9.2721  0.4958  0.0157  0.0039   3.1745  0.0584   \n",
       "3     1.4882 -0.0124  ...   8.5831  0.4990  0.0103  0.0025   2.0544  0.0202   \n",
       "4     1.5031 -0.0031  ...  10.9698  0.4800  0.4766  0.1045  99.3032  0.0202   \n",
       "...      ...     ...  ...      ...     ...     ...     ...      ...     ...   \n",
       "1562  1.3424 -0.0045  ...  11.7256  0.4988  0.0143  0.0039   2.8669  0.0068   \n",
       "1563  1.4333 -0.0061  ...  17.8379  0.4975  0.0131  0.0036   2.6238  0.0068   \n",
       "1564     NaN     NaN  ...  17.7267  0.4987  0.0153  0.0041   3.0590  0.0197   \n",
       "1565  1.4622 -0.0072  ...  19.2104  0.5004  0.0178  0.0038   3.5662  0.0262   \n",
       "1566     NaN     NaN  ...  22.9183  0.4987  0.0181  0.0040   3.6275  0.0117   \n",
       "\n",
       "         587     588       589  Pass/Fail  \n",
       "0        NaN     NaN       NaN         -1  \n",
       "1     0.0201  0.0060  208.2045         -1  \n",
       "2     0.0484  0.0148   82.8602          1  \n",
       "3     0.0149  0.0044   73.8432         -1  \n",
       "4     0.0149  0.0044   73.8432         -1  \n",
       "...      ...     ...       ...        ...  \n",
       "1562  0.0138  0.0047  203.1720         -1  \n",
       "1563  0.0138  0.0047  203.1720         -1  \n",
       "1564  0.0086  0.0025   43.5231         -1  \n",
       "1565  0.0245  0.0075   93.4941         -1  \n",
       "1566  0.0162  0.0045  137.7844         -1  \n",
       "\n",
       "[1567 rows x 539 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 #52 columns have been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "045a1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's replace the NaN value using median values.\n",
    "for i in df1.columns:\n",
    "    df1[i].fillna(df1[i].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0732ab64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    539\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isna().any().value_counts() #No NaN values are now presnt in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28d4ed3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 539)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape #Final shape of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72f7405e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dupes = df1.duplicated()\n",
    "sum(dupes) #0 duplicate records are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe138f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1567 entries, 0 to 1566\n",
      "Columns: 539 entries, 0 to Pass/Fail\n",
      "dtypes: float64(538), int64(1)\n",
      "memory usage: 6.4 MB\n"
     ]
    }
   ],
   "source": [
    "df1.info() #All of the variables are of numerical datatypes, therefore assuming that there are no abnormal records, otherwise the datatype would have been 'object'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02a79efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's very difficult to explore these many number of variables for outliers. Therefore directly replacing the outlier values using the whisker values of those variables.\n",
    "from scipy.stats import zscore #Scaling the data first.\n",
    "df1_scaled = df1.drop('Pass/Fail',axis=1).apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5371aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    422\n",
      "True     116\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As there are null values after finding z-score of each variable,\\nit means that there are duplicated values only in those columns, therefore we need to drop those columns'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df1_scaled.isna().any().value_counts())\n",
    "'''As there are null values after finding z-score of each variable,\n",
    "it means that there are duplicated values only in those columns, therefore we need to drop those columns'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fde5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_nan=[]\n",
    "for i in df1_scaled.columns:\n",
    "    if df1_scaled[i].isna().any():\n",
    "        list_nan.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b197e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_scaled=df1_scaled.drop(list_nan,axis=1)\n",
    "df2=df1.drop(list_nan,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b019bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop('Pass/Fail',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f0da8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df3.quantile(0.25)\n",
    "Q3 = df3.quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2df85cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace every outlier on the lower side by the lower whisker\n",
    "for i, j in zip(np.where(df3 < Q1 - 1.5 * IQR)[0], np.where(df3 < Q1 - 1.5 * IQR)[1]): \n",
    "    \n",
    "    whisker  = Q1 - 1.5 * IQR\n",
    "    df3.iloc[i,j] = whisker[j]\n",
    "    \n",
    "    \n",
    "#Replace every outlier on the upper side by the upper whisker    \n",
    "for i, j in zip(np.where(df3 > Q3 + 1.5 * IQR)[0], np.where(df3> Q3 + 1.5 * IQR)[1]):\n",
    "    \n",
    "    whisker  = Q3 + 1.5 * IQR\n",
    "    df3.iloc[i,j] = whisker[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54f400af",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaceStruct = {'Pass/Fail':{-1:0}}\n",
    "df2=df2.replace(replaceStruct) #Replacing the -1 value from the target variable to 0.\n",
    "target = df2['Pass/Fail'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3ba5bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Pass/Fail'] = target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77b41bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    413\n",
       "True      10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.apply(zscore).isna().any().value_counts() #Still there are null values found, which means after transformation there are same values present in some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1da0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_nan=[]\n",
    "df3_scaled=df3.drop('Pass/Fail',axis=1).apply(zscore)\n",
    "for i in df3_scaled.columns:\n",
    "    if df3_scaled[i].isna().any():\n",
    "        list_nan.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7aa4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.drop(list_nan,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08a96c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 413)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape #Thus, this dataset will be the final dataset which can be used to train the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66ec9b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are many ways, in which different types of classification algorithms can be trained on this dataset.\\n   First of all, we can go without target balancing or with target balancing.\\n   Then there are 2 ways from the point of view of feature extraction/elimination.\\n     1. Using the entire dataset as it is, which will take longer time to train the algorithms, but in turn might give us better accuracy\\n     2. Using principal component analysis (PCA) \\n   Then there are 4 different ways in which cross validation can be employed here, for each of the above listed way.\\n     1. Without using cross validation and without any hyperparameter tuning\\n     2. Using k fold cross validation without any hyperparameter tuning\\n     3. Using leave one out cross validation (LOOCV) without any hyperparameter tuning\\n     3. Using grid search CV\\n     4. Using randomized search CV\\n   At last, we'll use 3 different classification algorithms in each of the above listed ways.\\n     1. Logistic regression\\n     2. SVM\\n     3. Random Forest\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''There are many ways, in which different types of classification algorithms can be trained on this dataset.\n",
    "   First of all, we can go without target balancing or with target balancing.\n",
    "   Then there are 2 ways from the point of view of feature extraction/elimination.\n",
    "     1. Using the entire dataset as it is, which will take longer time to train the algorithms, but in turn might give us better accuracy\n",
    "     2. Using principal component analysis (PCA) \n",
    "   Then there are 4 different ways in which cross validation can be employed here, for each of the above listed way.\n",
    "     1. Without using cross validation and without any hyperparameter tuning\n",
    "     2. Using k fold cross validation without any hyperparameter tuning\n",
    "     3. Using leave one out cross validation (LOOCV) without any hyperparameter tuning\n",
    "     3. Using grid search CV\n",
    "     4. Using randomized search CV\n",
    "   At last, we'll use 3 different classification algorithms in each of the above listed ways.\n",
    "     1. Logistic regression\n",
    "     2. SVM\n",
    "     3. Random Forest'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16264319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's go with using the entire dataset first, without target balancing, without using PCA.\n",
    "#Importing the classification algorithms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = df3.drop('Pass/Fail',axis=1)\n",
    "y = df3['Pass/Fail'] #Separating the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a5121e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #Splitting the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec9de047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of 1s in whole dataset :  6.637 %\n",
      "% of 0s in whole dataset :  93.363 %\n",
      "% of 1s in training dataset :  6.569 %\n",
      "% of 0s in training dataset :  93.431 %\n",
      "% of 1s in test dataset :  6.794 %\n",
      "% of 0s in test dataset :  93.206 %\n"
     ]
    }
   ],
   "source": [
    "print(\"% of 1s in whole dataset : \",np.round(((len(df3[df3['Pass/Fail'] == 1]))/len(df3))*100,3),\"%\")\n",
    "print(\"% of 0s in whole dataset : \",np.round(((len(df3[df3['Pass/Fail'] == 0]))/len(df3))*100,3),\"%\")\n",
    "print(\"% of 1s in training dataset : \",np.round(((len(y_train[y_train == 1]))/len(y_train))*100,3),\"%\")\n",
    "print(\"% of 0s in training dataset : \",np.round(((len(y_train[y_train == 0]))/len(y_train))*100,3),\"%\")\n",
    "print(\"% of 1s in test dataset : \",np.round(((len(y_test[y_test == 1]))/len(y_test))*100,3),\"%\")\n",
    "print(\"% of 0s in test dataset : \",np.round(((len(y_test[y_test == 0]))/len(y_test))*100,3),\"%\")\n",
    "#More or less similar distribution of target variable in the entire dataset along with training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66c4d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using pipeline for the following algorithms\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pipeline_logreg = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('logreg', LogisticRegression())])\n",
    "pipeline_svc = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('clf_svc', svm.SVC())])\n",
    "pipeline_rf = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('clf_rf',RandomForestClassifier(n_estimators = 50, random_state=1))])\n",
    "#Instantiating the algorithms at their default hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfd3926f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('clf_rf',\n",
       "                 RandomForestClassifier(n_estimators=50, random_state=1))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_logreg.fit(X_train,y_train)\n",
    "pipeline_svc.fit(X_train,y_train)\n",
    "pipeline_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5e18f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "#We'll use training and test accuracies of each model that we'll be training, along with its f1 score, in order to evaluate every model\n",
    "'''We'll store following parameters in a dataframe in order to analyse the final outcomes of all of the models conveniently :-\n",
    "   1. Target balancing\n",
    "   2. PCA\n",
    "   3. CV\n",
    "   4. Algorithm\n",
    "   5. Training accuracy\n",
    "   6. Test Accuracy\n",
    "   7. F1 score'''\n",
    "df_ev = pd.DataFrame()\n",
    "m=1\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'no',\n",
    "                        'CV':'no',\n",
    "                        'Algorithm':'logistic',\n",
    "                        \"Train_acc\":pipeline_logreg.score(X_train,y_train),\n",
    "                        \"Test_acc\":pipeline_logreg.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=pipeline_logreg.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'no',\n",
    "                        'CV':'no',\n",
    "                        'Algorithm':'svc',\n",
    "                        \"Train_acc\":pipeline_svc.score(X_train,y_train),\n",
    "                        \"Test_acc\":pipeline_svc.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=pipeline_svc.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'no',\n",
    "                        'CV':'no',\n",
    "                        'Algorithm':'random_forest',\n",
    "                        \"Train_acc\":pipeline_rf.score(X_train,y_train),\n",
    "                        \"Test_acc\":pipeline_rf.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=pipeline_rf.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abdba844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>CV</th>\n",
       "      <th>PCA</th>\n",
       "      <th>Target_bal</th>\n",
       "      <th>Test_acc</th>\n",
       "      <th>Train_acc</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0.908705</td>\n",
       "      <td>0.997263</td>\n",
       "      <td>0.31746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svc</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.939781</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.999088</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Algorithm  CV PCA Target_bal  Test_acc Train_acc f1_score\n",
       "1       logistic  no  no         no  0.908705  0.997263  0.31746\n",
       "2            svc  no  no         no  0.932059  0.939781      0.0\n",
       "3  random_forest  no  no         no  0.932059  0.999088      0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28dd5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's go for k fold cross validation.\n",
    "X1_train = X_train.copy()\n",
    "y1_train = y_train.copy()\n",
    "X1_test = X_test.copy()\n",
    "y1_test = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fe880a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=4\n",
    "#Let's take 10 folds here.\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=10)\n",
    "l1_train_acc=[]\n",
    "l1_test_acc=[]\n",
    "l1_test_f1=[]\n",
    "for train,val in kfold.split(X1_train): #Training the svm classifier algorithm on 10 sets of training dataset only, in order to avoid data leakage\n",
    "    X11_train,X11_val = X1_train.iloc[train],X1_train.iloc[val]\n",
    "    y11_train,y11_val = y1_train.iloc[train],y1_train.iloc[val]\n",
    "    pipeline_svc = Pipeline([('scaler',StandardScaler()),\n",
    "                             ('clf_svc', svm.SVC())])\n",
    "    pipeline_svc.fit(X11_train,y11_train)\n",
    "    l1_train_acc.append(pipeline_svc.score(X11_train,y11_train))\n",
    "    l1_test_acc.append(pipeline_svc.score(X1_test,y1_test))\n",
    "    y1_pred_test = pipeline_svc.predict(X1_test)\n",
    "    l1_test_f1.append(f1_score(y_true=y1_test,y_pred=y1_pred_test))\n",
    "\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'no',\n",
    "                        'CV':'k-Fold',\n",
    "                        'Algorithm':'svc',\n",
    "                        \"Train_acc\":np.mean(l1_train_acc),\n",
    "                        \"Test_acc\":np.mean(l1_test_acc),\n",
    "                        \"f1_score\":np.mean(l1_test_f1)}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04378059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "m=5\n",
    "#Let's go for logistic regression classifier using k fold cross validation\n",
    "l1_train_acc=[]\n",
    "l1_test_acc=[]\n",
    "l1_test_f1=[]\n",
    "for train,val in kfold.split(X1_train):\n",
    "    X11_train,X11_val = X1_train.iloc[train],X1_train.iloc[val]\n",
    "    y11_train,y11_val = y1_train.iloc[train],y1_train.iloc[val]\n",
    "    pipeline_logreg = Pipeline([('scaler',StandardScaler()),\n",
    "                                ('logreg', LogisticRegression())])\n",
    "    pipeline_logreg.fit(X11_train,y11_train)\n",
    "    l1_train_acc.append(pipeline_logreg.score(X11_train,y11_train))\n",
    "    l1_test_acc.append(pipeline_logreg.score(X1_test,y1_test))\n",
    "    y1_pred_test = pipeline_logreg.predict(X1_test)\n",
    "    l1_test_f1.append(f1_score(y_true=y1_test,y_pred=y1_pred_test))\n",
    "\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'no',\n",
    "                        'CV':'k-Fold',\n",
    "                        'Algorithm':'logistic',\n",
    "                        \"Train_acc\":np.mean(l1_train_acc),\n",
    "                        \"Test_acc\":np.mean(l1_test_acc),\n",
    "                        \"f1_score\":np.mean(l1_test_f1)}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75a12eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's go for random forests classifier using k fold cross validation\n",
    "l1_train_acc=[]\n",
    "l1_test_acc=[]\n",
    "l1_test_f1=[]\n",
    "for train,val in kfold.split(X1_train):\n",
    "    X11_train,X11_val = X1_train.iloc[train],X1_train.iloc[val]\n",
    "    y11_train,y11_val = y1_train.iloc[train],y1_train.iloc[val]\n",
    "    pipeline_rf = Pipeline([('scaler',StandardScaler()),\n",
    "                            ('clf_rf', RandomForestClassifier(n_estimators = 50, random_state=1))])\n",
    "    pipeline_rf.fit(X11_train,y11_train)\n",
    "    l1_train_acc.append(pipeline_rf.score(X11_train,y11_train))\n",
    "    l1_test_acc.append(pipeline_rf.score(X1_test,y1_test))\n",
    "    y1_pred_test = pipeline_rf.predict(X1_test)\n",
    "    l1_test_f1.append(f1_score(y_true=y1_test,y_pred=y1_pred_test))\n",
    "\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'no',\n",
    "                        'CV':'k-Fold',\n",
    "                        'Algorithm':'random_forest',\n",
    "                        \"Train_acc\":np.mean(l1_train_acc),\n",
    "                        \"Test_acc\":np.mean(l1_test_acc),\n",
    "                        \"f1_score\":np.mean(l1_test_f1)}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f1ab43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Algorithm      CV PCA Target_bal  Test_acc Train_acc  f1_score\n",
      "1       logistic      no  no         no  0.908705  0.997263   0.31746\n",
      "2            svc      no  no         no  0.932059  0.939781       0.0\n",
      "3  random_forest      no  no         no  0.932059  0.999088       0.0\n",
      "4            svc  k-Fold  no         no  0.932059   0.93897       0.0\n",
      "5       logistic  k-Fold  no         no  0.901486  0.999595  0.266502\n",
      "6  random_forest  k-Fold  no         no  0.932059  0.998986       0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We can see that only cross validation without hyperparameter tuning leads to poorer results than the previous models\\nand a higher degree of overfitting can be seen here'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_ev) \n",
    "'''We can see that only cross validation without hyperparameter tuning leads to poorer results than the previous models\n",
    "and a higher degree of overfitting can be seen here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e41b87d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We can also try leave one out cross validation without hyperparameter tuning here, but as it'll take very long time to execute\\n   and the results will be approximately equal to the previous models without cv. Therefore we'll not go for LOOCV models here.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We can also try leave one out cross validation without hyperparameter tuning here, but as it'll take very long time to execute\n",
    "   and the results will be approximately equal to the previous models without cv. Therefore we'll not go for LOOCV models here.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2ee0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's go for randomized search CV first, in order to find the approximate range of the hyperparameters that give us best fit model.\n",
    "#then we can use grid search CV to find out the exact set of hyperparameters according to the range found out by using the previous CV.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "pipeline_logreg = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('logreg', LogisticRegression())])\n",
    "pipeline_svc = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('clf_svc', svm.SVC())])\n",
    "pipeline_rf = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('clf_rf',RandomForestClassifier(n_estimators = 50, random_state=1))])\n",
    "param_grid_svc = {'clf_svc__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                  'clf_svc__gamma': [0.001, 0.01, 0.1, 1, 10], \n",
    "                  'clf_svc__kernel':['rbf','poly']}\n",
    "param_grid_logreg = {'logreg__solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                     'logreg__penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "                     'logreg__C':[0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "param_grid_rf={'clf_rf__max_features': max_features,\n",
    "               'clf_rf__max_depth': max_depth,\n",
    "               'clf_rf__min_samples_split': min_samples_split,\n",
    "               'clf_rf__min_samples_leaf': min_samples_leaf,\n",
    "               'clf_rf__bootstrap': bootstrap,\n",
    "               \"clf_rf__criterion\": [\"gini\", \"entropy\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c88fea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomCV_logreg = RandomizedSearchCV(pipeline_logreg, param_distributions=param_grid_logreg, n_iter=50)\n",
    "randomCV_svc = RandomizedSearchCV(pipeline_svc, param_distributions=param_grid_svc, n_iter=50)\n",
    "randomCV_rf = RandomizedSearchCV(pipeline_rf, param_distributions=param_grid_rf, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "812e7f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "110 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.85674139        nan 0.87134911 0.85492736\n",
      "        nan 0.85674139 0.86860938        nan        nan 0.93430884\n",
      " 0.85492736 0.86131175 0.85674139        nan 0.85492736 0.92882939\n",
      "        nan 0.85674139        nan 0.91150685        nan 0.86861353\n",
      "        nan 0.93248236        nan        nan        nan        nan\n",
      " 0.77559568 0.86405147        nan 0.93430884        nan        nan\n",
      "        nan 0.89234122        nan        nan 0.87134911        nan\n",
      " 0.85492736 0.85674139 0.86131175 0.85674139 0.86405147 0.85674139\n",
      " 0.93430884 0.88320465]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                             ('clf_rf',\n",
       "                                              RandomForestClassifier(n_estimators=50,\n",
       "                                                                     random_state=1))]),\n",
       "                   n_iter=50,\n",
       "                   param_distributions={'clf_rf__bootstrap': [True, False],\n",
       "                                        'clf_rf__criterion': ['gini',\n",
       "                                                              'entropy'],\n",
       "                                        'clf_rf__max_depth': [10, 20, 30, 40,\n",
       "                                                              50, 60, 70, 80,\n",
       "                                                              90, 100, 110,\n",
       "                                                              None],\n",
       "                                        'clf_rf__max_features': ['auto',\n",
       "                                                                 'sqrt'],\n",
       "                                        'clf_rf__min_samples_leaf': [1, 2, 4],\n",
       "                                        'clf_rf__min_samples_split': [2, 5,\n",
       "                                                                      10]})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_logreg.fit(X1_train,y1_train)\n",
    "randomCV_svc.fit(X1_train,y1_train)\n",
    "randomCV_rf.fit(X1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0627130b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf_rf__min_samples_split': 10,\n",
       " 'clf_rf__min_samples_leaf': 4,\n",
       " 'clf_rf__max_features': 'auto',\n",
       " 'clf_rf__max_depth': 90,\n",
       " 'clf_rf__criterion': 'gini',\n",
       " 'clf_rf__bootstrap': False}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_rf.best_params_ #Listing out the general approximation of best possible hyperparameters for all 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0dc81c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf_svc__kernel': 'rbf', 'clf_svc__gamma': 1, 'clf_svc__C': 0.001}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93dd13f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg__solver': 'lbfgs', 'logreg__penalty': 'l2', 'logreg__C': 0.0001}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_logreg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73da92a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training accuracy of logistic regression model after randomized CV:  0.9343088418430885\n",
      "Best training accuracy of svc model after randomized CV:  0.9343088418430885\n",
      "Best training accuracy of random forest model after randomized CV:  0.9343088418430885\n"
     ]
    }
   ],
   "source": [
    "print('Best training accuracy of logistic regression model after randomized CV: ', randomCV_logreg.best_score_)\n",
    "print('Best training accuracy of svc model after randomized CV: ', randomCV_svc.best_score_)\n",
    "print('Best training accuracy of random forest model after randomized CV: ', randomCV_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "028d981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use the full grid search over the range near the above listed hyperparameters.\n",
    "param_grid_svc1 = {'clf_svc__C': [i for i in np.arange(0.0005,0.0035,0.0005)],\n",
    "                  'clf_svc__gamma': [i for i in np.arange(0.0005,0.0035,0.0005)], \n",
    "                  'clf_svc__kernel':['rbf','poly']}\n",
    "param_grid_logreg1 = {'logreg__solver': ['lbfgs', 'liblinear'],\n",
    "                     'logreg__penalty': ['l1', 'l2'],\n",
    "                     'logreg__C':[i for i in np.arange(0.000005,0.000035,0.000005)]}\n",
    "param_grid_rf1={'clf_rf__max_features':  ['auto', 'sqrt'],\n",
    "               'clf_rf__max_depth': max_depth,\n",
    "               'clf_rf__min_samples_split': min_samples_split,\n",
    "               'clf_rf__min_samples_leaf': min_samples_leaf,\n",
    "               'clf_rf__bootstrap': bootstrap,\n",
    "               \"clf_rf__criterion\": [\"gini\", \"entropy\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0adeb694",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_logreg = GridSearchCV( pipeline_logreg , param_grid = param_grid_logreg1, cv = 10)\n",
    "grid_svc = GridSearchCV( pipeline_svc , param_grid = param_grid_svc1, cv = 10)\n",
    "grid_rf = GridSearchCV( pipeline_rf , param_grid = param_grid_rf1, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ea51351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grid_logreg.fit(X_train,y_train)\\ngrid_svc.fit(X_train,y_train)\\ngrid_rf.fit(X_train,y_train)'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''grid_logreg.fit(X_train,y_train)\n",
    "grid_svc.fit(X_train,y_train)\n",
    "grid_rf.fit(X_train,y_train)'''\n",
    "#Extremely time consuming complex algorithms, therefore it is impossible to run these models on the entire dataset.\n",
    "#Going for feature elimination using PCA now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3c0aea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=300)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=300)\n",
    "pca.fit(X.apply(zscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10f3479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.step(list(range(1,301)),np.cumsum(pca.explained_variance_ratio_), where='mid')\n",
    "plt.ylabel('Cum of variation explained')\n",
    "plt.xlabel('eigen Value')\n",
    "\n",
    "plt.show() #As we can see in the plot that out of total 413 variables, their combination resulting into only 150 components can give us more than 90% explained variance of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4c287d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_logreg_p = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('pca',PCA(n_components=150)),\n",
    "    ('logreg', LogisticRegression())])\n",
    "pipeline_svc_p = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('pca',PCA(n_components=150)),\n",
    "    ('clf_svc', svm.SVC())])\n",
    "pipeline_rf_p = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('pca',PCA(n_components=150)),\n",
    "    ('clf_rf',RandomForestClassifier(n_estimators = 50, random_state=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a9b8fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=150)),\n",
       "                ('clf_rf',\n",
       "                 RandomForestClassifier(n_estimators=50, random_state=1))])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_logreg_p.fit(X_train,y_train)\n",
    "pipeline_svc_p.fit(X_train,y_train)\n",
    "pipeline_rf_p.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d2b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25809af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'no',\n",
    "                        'Algorithm':'logistic',\n",
    "                        \"Train_acc\":pipeline_logreg_p.score(X_train,y_train),\n",
    "                        \"Test_acc\":pipeline_logreg_p.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=pipeline_logreg_p.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'no',\n",
    "                        'Algorithm':'svc',\n",
    "                        \"Train_acc\":pipeline_svc_p.score(X_train,y_train),\n",
    "                        \"Test_acc\":pipeline_svc_p.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=pipeline_svc_p.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'no',\n",
    "                        'Algorithm':'random_forest',\n",
    "                        \"Train_acc\":pipeline_rf_p.score(X_train,y_train),\n",
    "                        \"Test_acc\":pipeline_rf_p.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=pipeline_rf_p.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b4806fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10)\n",
    "l1_train_acc=[]\n",
    "l1_test_acc=[]\n",
    "l1_test_f1=[]\n",
    "for train,val in kfold.split(X1_train): #Training the svm classifier algorithm on 10 sets of training dataset only, in order to avoid data leakage\n",
    "    X11_train,X11_val = X1_train.iloc[train],X1_train.iloc[val]\n",
    "    y11_train,y11_val = y1_train.iloc[train],y1_train.iloc[val]\n",
    "    pipeline_svc = Pipeline([('scaler',StandardScaler()),\n",
    "                             ('pca',PCA(n_components=150)),\n",
    "                             ('clf_svc', svm.SVC())])\n",
    "    pipeline_svc.fit(X11_train,y11_train)\n",
    "    l1_train_acc.append(pipeline_svc.score(X11_train,y11_train))\n",
    "    l1_test_acc.append(pipeline_svc.score(X1_test,y1_test))\n",
    "    y1_pred_test = pipeline_svc.predict(X1_test)\n",
    "    l1_test_f1.append(f1_score(y_true=y1_test,y_pred=y1_pred_test))\n",
    "\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'k-Fold',\n",
    "                        'Algorithm':'svc',\n",
    "                        \"Train_acc\":np.mean(l1_train_acc),\n",
    "                        \"Test_acc\":np.mean(l1_test_acc),\n",
    "                        \"f1_score\":np.mean(l1_test_f1)}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3cfb0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_train_acc=[]\n",
    "l1_test_acc=[]\n",
    "l1_test_f1=[]\n",
    "for train,val in kfold.split(X1_train):\n",
    "    X11_train,X11_val = X1_train.iloc[train],X1_train.iloc[val]\n",
    "    y11_train,y11_val = y1_train.iloc[train],y1_train.iloc[val]\n",
    "    pipeline_logreg = Pipeline([('scaler',StandardScaler()),\n",
    "                                ('pca',PCA(n_components=150)),\n",
    "                                ('logreg', LogisticRegression())])\n",
    "    pipeline_logreg.fit(X11_train,y11_train)\n",
    "    l1_train_acc.append(pipeline_logreg.score(X11_train,y11_train))\n",
    "    l1_test_acc.append(pipeline_logreg.score(X1_test,y1_test))\n",
    "    y1_pred_test = pipeline_logreg.predict(X1_test)\n",
    "    l1_test_f1.append(f1_score(y_true=y1_test,y_pred=y1_pred_test))\n",
    "\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'k-Fold',\n",
    "                        'Algorithm':'logistic',\n",
    "                        \"Train_acc\":np.mean(l1_train_acc),\n",
    "                        \"Test_acc\":np.mean(l1_test_acc),\n",
    "                        \"f1_score\":np.mean(l1_test_f1)}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fde8fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_train_acc=[]\n",
    "l1_test_acc=[]\n",
    "l1_test_f1=[]\n",
    "for train,val in kfold.split(X1_train):\n",
    "    X11_train,X11_val = X1_train.iloc[train],X1_train.iloc[val]\n",
    "    y11_train,y11_val = y1_train.iloc[train],y1_train.iloc[val]\n",
    "    pipeline_rf = Pipeline([('scaler',StandardScaler()),\n",
    "                            ('pca',PCA(n_components=150)),\n",
    "                            ('clf_rf', RandomForestClassifier(n_estimators = 50, random_state=1))])\n",
    "    pipeline_rf.fit(X11_train,y11_train)\n",
    "    l1_train_acc.append(pipeline_rf.score(X11_train,y11_train))\n",
    "    l1_test_acc.append(pipeline_rf.score(X1_test,y1_test))\n",
    "    y1_pred_test = pipeline_rf.predict(X1_test)\n",
    "    l1_test_f1.append(f1_score(y_true=y1_test,y_pred=y1_pred_test))\n",
    "\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'k-Fold',\n",
    "                        'Algorithm':'random_forest',\n",
    "                        \"Train_acc\":np.mean(l1_train_acc),\n",
    "                        \"Test_acc\":np.mean(l1_test_acc),\n",
    "                        \"f1_score\":np.mean(l1_test_f1)}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "82d43516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's go for randomized search CV on the PCA generated data\n",
    "randomCV_logreg_p = RandomizedSearchCV(pipeline_logreg_p, param_distributions=param_grid_logreg, n_iter=50)\n",
    "randomCV_svc_p = RandomizedSearchCV(pipeline_svc_p, param_distributions=param_grid_svc, n_iter=50)\n",
    "randomCV_rf_p = RandomizedSearchCV(pipeline_rf_p, param_distributions=param_grid_rf, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c9359d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "125 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.84035699        nan        nan 0.89964301        nan        nan\n",
      " 0.93430884 0.83851391        nan 0.93248236        nan        nan\n",
      "        nan 0.93248236 0.8832254  0.84673308        nan 0.93430884\n",
      "        nan 0.90146949        nan        nan 0.93430884        nan\n",
      "        nan        nan 0.83850976 0.8385056         nan        nan\n",
      " 0.83850976        nan 0.84853051        nan        nan 0.93430884\n",
      " 0.90055625 0.84856372        nan        nan 0.82847655 0.93430884\n",
      "        nan 0.83395185 0.91422997 0.84581154 0.85950602        nan\n",
      "        nan 0.83577833]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                             ('pca', PCA(n_components=150)),\n",
       "                                             ('clf_rf',\n",
       "                                              RandomForestClassifier(n_estimators=50,\n",
       "                                                                     random_state=1))]),\n",
       "                   n_iter=50,\n",
       "                   param_distributions={'clf_rf__bootstrap': [True, False],\n",
       "                                        'clf_rf__criterion': ['gini',\n",
       "                                                              'entropy'],\n",
       "                                        'clf_rf__max_depth': [10, 20, 30, 40,\n",
       "                                                              50, 60, 70, 80,\n",
       "                                                              90, 100, 110,\n",
       "                                                              None],\n",
       "                                        'clf_rf__max_features': ['auto',\n",
       "                                                                 'sqrt'],\n",
       "                                        'clf_rf__min_samples_leaf': [1, 2, 4],\n",
       "                                        'clf_rf__min_samples_split': [2, 5,\n",
       "                                                                      10]})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_logreg_p.fit(X1_train,y1_train)\n",
    "randomCV_svc_p.fit(X1_train,y1_train)\n",
    "randomCV_rf_p.fit(X1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b08bcc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg__solver': 'liblinear', 'logreg__penalty': 'l1', 'logreg__C': 0.0001}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_logreg_p.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e5bd5e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf_svc__kernel': 'rbf', 'clf_svc__gamma': 0.001, 'clf_svc__C': 0.001}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_svc_p.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2fac1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf_rf__min_samples_split': 2,\n",
       " 'clf_rf__min_samples_leaf': 4,\n",
       " 'clf_rf__max_features': 'sqrt',\n",
       " 'clf_rf__max_depth': 20,\n",
       " 'clf_rf__criterion': 'gini',\n",
       " 'clf_rf__bootstrap': True}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_rf_p.best_params_ #Difference can be seen between the best parameters obtained by randomized search before and after PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24705016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training accuracy of logistic regression model after randomized CV:  0.9343088418430885\n",
      "Best training accuracy of svc model after randomized CV:  0.9343088418430885\n",
      "Best training accuracy of random forest model after randomized CV:  0.9343088418430885\n"
     ]
    }
   ],
   "source": [
    "print('Best training accuracy of logistic regression model after randomized CV: ', randomCV_logreg_p.best_score_)\n",
    "print('Best training accuracy of svc model after randomized CV: ', randomCV_svc_p.best_score_)\n",
    "print('Best training accuracy of random forest model after randomized CV: ', randomCV_rf_p.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f686c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use the full grid search over the range near the above listed hyperparameters.\n",
    "param_grid_svc_p = {'clf_svc__C': [i for i in np.arange(0.0005,0.0025,0.0005)],\n",
    "                  'clf_svc__gamma': [i for i in np.arange(0.005,0.025,0.005)], \n",
    "                  'clf_svc__kernel':['poly']}\n",
    "param_grid_logreg_p = {'logreg__solver': ['newton-cg'],\n",
    "                     'logreg__penalty': ['l2'],\n",
    "                     'logreg__C':[i for i in np.arange(0.000005,0.000025,0.000005)]}\n",
    "param_grid_rf_p = {'clf_rf__max_features':  ['sqrt'],\n",
    "               'clf_rf__max_depth': [60,65,70,75,80],\n",
    "               'clf_rf__min_samples_split': [4,5,6],\n",
    "               'clf_rf__min_samples_leaf': [1],\n",
    "               'clf_rf__bootstrap': [True],\n",
    "               \"clf_rf__criterion\": [\"gini\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd1d6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_logreg_p = GridSearchCV( pipeline_logreg_p , param_grid = param_grid_logreg_p, cv = 5)\n",
    "grid_svc_p = GridSearchCV( pipeline_svc_p , param_grid = param_grid_svc_p, cv = 5)\n",
    "grid_rf_p = GridSearchCV( pipeline_rf_p , param_grid = param_grid_rf_p, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d4f9fa47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('pca', PCA(n_components=150)),\n",
       "                                       ('clf_rf',\n",
       "                                        RandomForestClassifier(n_estimators=50,\n",
       "                                                               random_state=1))]),\n",
       "             param_grid={'clf_rf__bootstrap': [True],\n",
       "                         'clf_rf__criterion': ['gini'],\n",
       "                         'clf_rf__max_depth': [60, 65, 70, 75, 80],\n",
       "                         'clf_rf__max_features': ['sqrt'],\n",
       "                         'clf_rf__min_samples_leaf': [1],\n",
       "                         'clf_rf__min_samples_split': [4, 5, 6]})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_logreg_p.fit(X_train,y_train)\n",
    "grid_svc_p.fit(X_train,y_train)\n",
    "grid_rf_p.fit(X_train,y_train) #Fast execution due to feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9aa967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'GridSearch',\n",
    "                        'Algorithm':'random_forest',\n",
    "                        \"Train_acc\":grid_rf_p.score(X_train,y_train),\n",
    "                        \"Test_acc\":grid_rf_p.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=grid_rf_p.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'GridSearch',\n",
    "                        'Algorithm':'svc',\n",
    "                        \"Train_acc\":grid_svc_p.score(X_train,y_train),\n",
    "                        \"Test_acc\":grid_svc_p.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=grid_svc_p.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'no',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'GridSearch',\n",
    "                        'Algorithm':'logreg',\n",
    "                        \"Train_acc\":grid_logreg_p.score(X_train,y_train),\n",
    "                        \"Test_acc\":grid_logreg_p.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=grid_logreg_p.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "33da82e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>CV</th>\n",
       "      <th>PCA</th>\n",
       "      <th>Target_bal</th>\n",
       "      <th>Test_acc</th>\n",
       "      <th>Train_acc</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0.908705</td>\n",
       "      <td>0.997263</td>\n",
       "      <td>0.31746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svc</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.939781</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.999088</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>svc</td>\n",
       "      <td>k-Fold</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.93897</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logistic</td>\n",
       "      <td>k-Fold</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0.901486</td>\n",
       "      <td>0.999595</td>\n",
       "      <td>0.266502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>k-Fold</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.998986</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>logistic</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0.908705</td>\n",
       "      <td>0.961679</td>\n",
       "      <td>0.271186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>svc</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.938869</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.998175</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>svc</td>\n",
       "      <td>k-Fold</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.938463</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>logistic</td>\n",
       "      <td>k-Fold</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0.898514</td>\n",
       "      <td>0.968876</td>\n",
       "      <td>0.251833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>k-Fold</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.995439</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>GridSearch</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.994526</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>svc</td>\n",
       "      <td>GridSearch</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.934307</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>logreg</td>\n",
       "      <td>GridSearch</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0.932059</td>\n",
       "      <td>0.934307</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Algorithm          CV  PCA Target_bal  Test_acc Train_acc  f1_score\n",
       "1        logistic          no   no         no  0.908705  0.997263   0.31746\n",
       "2             svc          no   no         no  0.932059  0.939781       0.0\n",
       "3   random_forest          no   no         no  0.932059  0.999088       0.0\n",
       "4             svc      k-Fold   no         no  0.932059   0.93897       0.0\n",
       "5        logistic      k-Fold   no         no  0.901486  0.999595  0.266502\n",
       "6   random_forest      k-Fold   no         no  0.932059  0.998986       0.0\n",
       "7        logistic          no  yes         no  0.908705  0.961679  0.271186\n",
       "8             svc          no  yes         no  0.932059  0.938869       0.0\n",
       "9   random_forest          no  yes         no  0.932059  0.998175       0.0\n",
       "10            svc      k-Fold  yes         no  0.932059  0.938463       0.0\n",
       "11       logistic      k-Fold  yes         no  0.898514  0.968876  0.251833\n",
       "12  random_forest      k-Fold  yes         no  0.932059  0.995439       0.0\n",
       "13  random_forest  GridSearch  yes         no  0.932059  0.994526       0.0\n",
       "14            svc  GridSearch  yes         no  0.932059  0.934307       0.0\n",
       "15         logreg  GridSearch  yes         no  0.932059  0.934307       0.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ev #We can see that the f1 metric is zero in many cases, which means that the resultant models are highly biased towards predicting the target variable as zeroes.\n",
    "#Let's try target balancing in this case, by using imblearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de6ae3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE #Let's try upsampling in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8561e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(sampling_strategy = 1 ,k_neighbors = 5, random_state=1)   #Synthetic Minority Over Sampling Technique\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3016df0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before UpSampling, counts of label '1': 72\n",
      "Before UpSampling, counts of label '0': 1024 \n",
      "\n",
      "After UpSampling, counts of label '1': 1024\n",
      "After UpSampling, counts of label '0': 1024 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Before UpSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before UpSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
    "#Only 72 1s.\n",
    "print(\"After UpSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After UpSampling, counts of label '0': {} \\n\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aa54ecc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=150)),\n",
       "                ('clf_rf',\n",
       "                 RandomForestClassifier(n_estimators=50, random_state=1))])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Directly going for PCA version of the data to train the upsampled data, in order to reduce the computation time\n",
    "pipeline_logreg_p.fit(X_train_res,y_train_res)\n",
    "pipeline_svc_p.fit(X_train_res,y_train_res)\n",
    "pipeline_rf_p.fit(X_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b0d8a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g = pd.DataFrame({m:{\"Target_bal\":'yes',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'no',\n",
    "                        'Algorithm':'logistic',\n",
    "                        \"Train_acc\":pipeline_logreg_p.score(X_train_res,y_train_res),\n",
    "                        \"Test_acc\":pipeline_logreg_p.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=pipeline_logreg_p.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'yes',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'no',\n",
    "                        'Algorithm':'svc',\n",
    "                        \"Train_acc\":pipeline_svc_p.score(X_train_res,y_train_res),\n",
    "                        \"Test_acc\":pipeline_svc_p.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=pipeline_svc_p.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'yes',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'no',\n",
    "                        'Algorithm':'random_forest',\n",
    "                        \"Train_acc\":pipeline_rf_p.score(X_train_res,y_train_res),\n",
    "                        \"Test_acc\":pipeline_rf_p.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=pipeline_rf_p.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93f0993c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "2043    1\n",
       "2044    1\n",
       "2045    1\n",
       "2046    1\n",
       "2047    1\n",
       "Length: 2048, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb26db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_train_acc=[]\n",
    "l1_test_acc=[]\n",
    "l1_test_f1=[]\n",
    "for train,val in kfold.split(X_train_res): #Training the svm classifier algorithm on 10 sets of training dataset only, in order to avoid data leakage\n",
    "    X11_train,X11_val = X_train_res.iloc[train],X_train_res.iloc[val]\n",
    "    y11_train,y11_val = pd.Series(y_train_res).iloc[train],pd.Series(y_train_res).iloc[val]\n",
    "    pipeline_svc = Pipeline([('scaler',StandardScaler()),\n",
    "                             ('pca',PCA(n_components=150)),\n",
    "                             ('clf_svc', svm.SVC())])\n",
    "    pipeline_svc.fit(X11_train,y11_train)\n",
    "    l1_train_acc.append(pipeline_svc.score(X_train_res,y_train_res))\n",
    "    l1_test_acc.append(pipeline_svc.score(X1_test,y1_test))\n",
    "    y1_pred_test = pipeline_svc.predict(X1_test)\n",
    "    l1_test_f1.append(f1_score(y_true=y1_test,y_pred=y1_pred_test))\n",
    "\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'yes',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'k-Fold',\n",
    "                        'Algorithm':'svc',\n",
    "                        \"Train_acc\":np.mean(l1_train_acc),\n",
    "                        \"Test_acc\":np.mean(l1_test_acc),\n",
    "                        \"f1_score\":np.mean(l1_test_f1)}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f756430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "l1_train_acc=[]\n",
    "l1_test_acc=[]\n",
    "l1_test_f1=[]\n",
    "for train,val in kfold.split(X_train_res): \n",
    "    X11_train,X11_val = X_train_res.iloc[train],X_train_res.iloc[val]\n",
    "    y11_train,y11_val = pd.Series(y_train_res).iloc[train],pd.Series(y_train_res).iloc[val]\n",
    "    pipeline_logreg = Pipeline([('scaler',StandardScaler()),\n",
    "                                ('pca',PCA(n_components=150)),\n",
    "                                ('logreg', LogisticRegression())])\n",
    "    pipeline_logreg.fit(X11_train,y11_train)\n",
    "    l1_train_acc.append(pipeline_logreg.score(X_train_res,y_train_res))\n",
    "    l1_test_acc.append(pipeline_logreg.score(X1_test,y1_test))\n",
    "    y1_pred_test = pipeline_logreg.predict(X1_test)\n",
    "    l1_test_f1.append(f1_score(y_true=y1_test,y_pred=y1_pred_test))\n",
    "\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'yes',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'k-Fold',\n",
    "                        'Algorithm':'logistic',\n",
    "                        \"Train_acc\":np.mean(l1_train_acc),\n",
    "                        \"Test_acc\":np.mean(l1_test_acc),\n",
    "                        \"f1_score\":np.mean(l1_test_f1)}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0bd2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_train_acc=[]\n",
    "l1_test_acc=[]\n",
    "l1_test_f1=[]\n",
    "for train,val in kfold.split(X_train_res): \n",
    "    X11_train,X11_val = X_train_res.iloc[train],X_train_res.iloc[val]\n",
    "    y11_train,y11_val = pd.Series(y_train_res).iloc[train],pd.Series(y_train_res).iloc[val]\n",
    "    pipeline_rf = Pipeline([('scaler',StandardScaler()),\n",
    "                            ('pca',PCA(n_components=150)),\n",
    "                            ('clf_rf', RandomForestClassifier(n_estimators = 50, random_state=1))])\n",
    "    pipeline_rf.fit(X11_train,y11_train)\n",
    "    l1_train_acc.append(pipeline_rf.score(X_train_res,y_train_res))\n",
    "    l1_test_acc.append(pipeline_rf.score(X1_test,y1_test))\n",
    "    y1_pred_test = pipeline_rf.predict(X1_test)\n",
    "    l1_test_f1.append(f1_score(y_true=y1_test,y_pred=y1_pred_test))\n",
    "\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'yes',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'k-Fold',\n",
    "                        'Algorithm':'random_forest',\n",
    "                        \"Train_acc\":np.mean(l1_train_acc),\n",
    "                        \"Test_acc\":np.mean(l1_test_acc),\n",
    "                        \"f1_score\":np.mean(l1_test_f1)}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "46be0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's go for randomized search CV on the PCA generated data\n",
    "randomCV_logreg_p = RandomizedSearchCV(pipeline_logreg_p, param_distributions=param_grid_logreg, n_iter=50)\n",
    "randomCV_svc_p = RandomizedSearchCV(pipeline_svc_p, param_distributions=param_grid_svc, n_iter=50)\n",
    "randomCV_rf_p = RandomizedSearchCV(pipeline_rf_p, param_distributions=param_grid_rf, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c4e7c613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "130 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.9238273  0.92335739 0.91992128        nan        nan 0.85790924\n",
      "        nan 0.89892659 0.93164649 0.5        0.91943229        nan\n",
      "        nan 0.9272479  0.93261852        nan        nan        nan\n",
      " 0.93311348 0.93017473        nan        nan 0.5               nan\n",
      "        nan        nan 0.85742024        nan 0.92724193 0.92578091\n",
      " 0.85644582        nan        nan        nan 0.92579164 0.79785914\n",
      "        nan        nan        nan        nan 0.93407836        nan\n",
      "        nan 0.79834814        nan        nan        nan 0.78175443\n",
      " 0.91699088 0.92870773]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                             ('pca', PCA(n_components=150)),\n",
       "                                             ('clf_rf',\n",
       "                                              RandomForestClassifier(n_estimators=50,\n",
       "                                                                     random_state=1))]),\n",
       "                   n_iter=50,\n",
       "                   param_distributions={'clf_rf__bootstrap': [True, False],\n",
       "                                        'clf_rf__criterion': ['gini',\n",
       "                                                              'entropy'],\n",
       "                                        'clf_rf__max_depth': [10, 20, 30, 40,\n",
       "                                                              50, 60, 70, 80,\n",
       "                                                              90, 100, 110,\n",
       "                                                              None],\n",
       "                                        'clf_rf__max_features': ['auto',\n",
       "                                                                 'sqrt'],\n",
       "                                        'clf_rf__min_samples_leaf': [1, 2, 4],\n",
       "                                        'clf_rf__min_samples_split': [2, 5,\n",
       "                                                                      10]})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_logreg_p.fit(X_train_res,y_train_res)\n",
    "randomCV_svc_p.fit(X_train_res,y_train_res)\n",
    "randomCV_rf_p.fit(X_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7a21357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg__solver': 'lbfgs', 'logreg__penalty': 'none', 'logreg__C': 1e-05}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_logreg_p.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "66dd7a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf_svc__kernel': 'rbf', 'clf_svc__gamma': 0.01, 'clf_svc__C': 1}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_svc_p.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "35b3dcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf_rf__min_samples_split': 2,\n",
       " 'clf_rf__min_samples_leaf': 1,\n",
       " 'clf_rf__max_features': 'auto',\n",
       " 'clf_rf__max_depth': 70,\n",
       " 'clf_rf__criterion': 'gini',\n",
       " 'clf_rf__bootstrap': True}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_rf_p.best_params_ #Again, different optimum hyperparameters are found out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "20f7b48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training accuracy of logistic regression model after randomized CV:  0.9340783588764984\n",
      "Best training accuracy of svc model after randomized CV:  1.0\n",
      "Best training accuracy of random forest model after randomized CV:  0.9995121951219513\n"
     ]
    }
   ],
   "source": [
    "print('Best training accuracy of logistic regression model after randomized CV: ', randomCV_logreg_p.best_score_)\n",
    "print('Best training accuracy of svc model after randomized CV: ', randomCV_svc_p.best_score_)\n",
    "print('Best training accuracy of random forest model after randomized CV: ', randomCV_rf_p.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ffabe5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use the full grid search over the range near the above listed hyperparameters.\n",
    "param_grid_svc_p = {'clf_svc__C': [i for i in np.arange(0.5,2.5,0.5)],\n",
    "                  'clf_svc__gamma': [i for i in np.arange(0.005,0.025,0.005)], \n",
    "                  'clf_svc__kernel':['rbf']}\n",
    "param_grid_logreg_p = {'logreg__solver': ['liblinear'],\n",
    "                     'logreg__penalty': ['l2'],\n",
    "                     'logreg__C':[i for i in np.arange(50,250,50)]}\n",
    "param_grid_rf_p = {'clf_rf__max_features':  ['auto'],\n",
    "               'clf_rf__max_depth': [80,90,100],\n",
    "               'clf_rf__min_samples_split': [4,5,6],\n",
    "               'clf_rf__min_samples_leaf': [2,3,4],\n",
    "               'clf_rf__bootstrap': [False],\n",
    "               \"clf_rf__criterion\": [\"gini\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6e39e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_logreg_p = GridSearchCV( pipeline_logreg_p , param_grid = param_grid_logreg_p, cv = 5)\n",
    "grid_svc_p = GridSearchCV( pipeline_svc_p , param_grid = param_grid_svc_p, cv = 5)\n",
    "grid_rf_p = GridSearchCV( pipeline_rf_p , param_grid = param_grid_rf_p, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca233929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('pca', PCA(n_components=150)),\n",
       "                                       ('clf_rf',\n",
       "                                        RandomForestClassifier(n_estimators=50,\n",
       "                                                               random_state=1))]),\n",
       "             param_grid={'clf_rf__bootstrap': [False],\n",
       "                         'clf_rf__criterion': ['gini'],\n",
       "                         'clf_rf__max_depth': [80, 90, 100],\n",
       "                         'clf_rf__max_features': ['auto'],\n",
       "                         'clf_rf__min_samples_leaf': [2, 3, 4],\n",
       "                         'clf_rf__min_samples_split': [4, 5, 6]})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_logreg_p.fit(X_train_res,y_train_res)\n",
    "grid_svc_p.fit(X_train_res,y_train_res)\n",
    "grid_rf_p.fit(X_train_res,y_train_res) #Fast execution due to feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6aa63e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g = pd.DataFrame({m:{\"Target_bal\":'yes',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'GridSearch',\n",
    "                        'Algorithm':'random_forest',\n",
    "                        \"Train_acc\":grid_rf_p.score(X_train_res,y_train_res),\n",
    "                        \"Test_acc\":grid_rf_p.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=grid_rf_p.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'yes',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'GridSearch',\n",
    "                        'Algorithm':'svc',\n",
    "                        \"Train_acc\":grid_svc_p.score(X_train_res,y_train_res),\n",
    "                        \"Test_acc\":grid_svc_p.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=grid_svc_p.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1\n",
    "df_g = pd.DataFrame({m:{\"Target_bal\":'yes',\n",
    "                        \"PCA\":'yes',\n",
    "                        'CV':'GridSearch',\n",
    "                        'Algorithm':'logreg',\n",
    "                        \"Train_acc\":grid_logreg_p.score(X_train_res,y_train_res),\n",
    "                        \"Test_acc\":grid_logreg_p.score(X_test,y_test),\n",
    "                        \"f1_score\":f1_score(y_true=y_test,y_pred=grid_logreg_p.predict(X_test))}}).transpose()\n",
    "df_ev = pd.concat([df_ev,df_g])\n",
    "m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3884e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ca78b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neglecting the higher execution timing and higher dimensionality, the first and foremost logistic regression model without target balancing, without PCA and without CV gives us best f1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8e5eab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('logreg', LogisticRegression())])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_logreg = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('logreg', LogisticRegression())])\n",
    "pipeline_logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "919a3e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9087048832271762"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_logreg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9b82fd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for testing data of the decision tree with Gini impurity as solving method.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x29f01ea9df0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Confusion Matrix for testing data of the decision tree with Gini impurity as solving method.\")\n",
    "pred_test = pipeline_logreg.predict(X_test)\n",
    "cm=metrics.confusion_matrix(y_test, pred_test)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3816cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the future file.\n",
    "df_f = pd.read_excel('Future_predictions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "768effb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df_f.drop('Time',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9a994370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df_f[[int(i) for i in X.columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d8208003",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f=df_f.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a0b1c08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_pred_fut = pipeline_logreg.predict(df_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "edd702d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_fut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ad49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Conclusion:-\n",
    "   while it is advisable to test various data simplification techniques such as feature elimination, target balancing, etc. along\n",
    "   with different cross validation techniques, it is observed in this case that such methods have only lowered the accuracy of the models\n",
    "   compared with that of the ones performed on the data after such techniques.\n",
    "   Even after target balancing, the f1 scores of svc and random forests models remained zero, which means that the models were still heavily biased toward\n",
    "   prediciting zeroes only.\n",
    "   Sometimes, as in this case, it is better to stick with the simpler models, without any cross valiation and other data simplication techniques.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
